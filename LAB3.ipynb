{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwNiUthT9S_T",
        "outputId": "ece56749-53d8-40d2-8b2b-bdeb52811c87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://dataflow-samples/shakespeare/macbeth.txt...\n",
            "/ [0 files][    0.0 B/102.9 KiB]                                                \r/ [1 files][102.9 KiB/102.9 KiB]                                                \r\n",
            "Operation completed over 1 objects/102.9 KiB.                                    \n"
          ]
        }
      ],
      "source": [
        "# Installing Apache Beam and calling the dataset.\n",
        "\n",
        "!pip install apache-beam[gcp] -q\n",
        "!mkdir -p data\n",
        "!gsutil cp gs://dataflow-samples/shakespeare/macbeth.txt data/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing apache beam and necessary data files.\n",
        "\n",
        "import apache_beam as beam\n",
        "import re\n",
        "import string\n",
        "\n",
        "inputs_pattern = 'data/macbeth.txt'\n",
        "outputs_top_names = 'outputs_macbeth_top/part'\n",
        "outputs_word_lengths = 'outputs_macbeth_lengths/part'"
      ],
      "metadata": {
        "id": "35lGBfET99rt"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Helper functions for the beam pipeline.\n",
        "\n",
        "def clean_token(token):\n",
        "    return token.strip(string.punctuation).capitalize()\n",
        "\n",
        "def is_likely_name(word):\n",
        "    ignore = {'The', 'This', 'That', 'From', 'With', 'When', 'Thou', 'Shall'}\n",
        "    return len(word) >= 4 and word not in ignore\n",
        "\n",
        "def word_length_pair(word):\n",
        "    cleaned = word.strip(string.punctuation)\n",
        "    if cleaned:\n",
        "        return (len(cleaned), 1)\n",
        "    else:\n",
        "        return (0, 0)\n",
        "with beam.Pipeline() as pipeline:\n",
        "    word_lengths = (\n",
        "        pipeline\n",
        "        | 'Read lines again' >> beam.io.ReadFromText(inputs_pattern)\n",
        "        | 'Extract capitalized words again' >> beam.FlatMap(lambda line: re.findall(r\"\\b[A-Z][a-zA-Z']+\\b\", line))\n",
        "        | 'Clean tokens again' >> beam.Map(clean_token)\n",
        "        | 'Filter likely names again' >> beam.Filter(is_likely_name)\n",
        "        | 'Pair lengths' >> beam.Map(word_length_pair)\n",
        "        | 'Filter valid lengths' >> beam.Filter(lambda x: x[0] > 0)\n",
        "        | 'Count by length' >> beam.CombinePerKey(sum)\n",
        "        | 'Format length output' >> beam.Map(lambda kv: f\"Length {kv[0]}: {kv[1]}\")\n",
        "        | 'Write lengths' >> beam.io.WriteToText(outputs_word_lengths)\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bbh42NOj-Bnp",
        "outputId": "a2e52ac6-1aee-4647-d4fc-9e6f83c1d671"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "\n",
        "    # To get top 10 Character Names\n",
        "    top_characters = (\n",
        "        pipeline\n",
        "        | 'Read lines' >> beam.io.ReadFromText(inputs_pattern)\n",
        "        | 'Extract capitalized words' >> beam.FlatMap(lambda line: re.findall(r\"\\b[A-Z][a-zA-Z']+\\b\", line))\n",
        "        | 'Clean words' >> beam.Map(clean_token)\n",
        "        | 'Filter likely names' >> beam.Filter(is_likely_name)\n",
        "        | 'Pair with 1' >> beam.Map(lambda name: (name, 1))\n",
        "        | 'Count occurrences' >> beam.CombinePerKey(sum)\n",
        "        | 'Get top 10' >> beam.combiners.Top.Of(10, key=lambda kv: kv[1])\n",
        "        | 'Flatten top 10' >> beam.FlatMap(lambda lst: lst)\n",
        "        | 'Format output' >> beam.Map(lambda kv: f\"{kv[0]}: {kv[1]}\")\n",
        "        | 'Write top names' >> beam.io.WriteToText(outputs_top_names)\n",
        "    )\n",
        "\n",
        "    # To get Word Length Distribution\n",
        "    word_lengths = (\n",
        "        pipeline\n",
        "        | 'Read lines again' >> beam.io.ReadFromText(inputs_pattern)\n",
        "        | 'Extract capitalized words again' >> beam.FlatMap(lambda line: re.findall(r\"\\b[A-Z][a-zA-Z']+\\b\", line))\n",
        "        | 'Clean tokens again' >> beam.Map(clean_token)\n",
        "        | 'Filter likely names again' >> beam.Filter(is_likely_name)\n",
        "        | 'Pair lengths' >> beam.Map(word_length_pair)\n",
        "        | 'Filter None lengths' >> beam.Filter(lambda x: x is not None)\n",
        "        | 'Count by length' >> beam.CombinePerKey(sum)\n",
        "        | 'Format length output' >> beam.Map(lambda kv: f\"Length {kv[0]}: {kv[1]}\")\n",
        "        | 'Write lengths' >> beam.io.WriteToText(outputs_word_lengths)\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5KUbh0o-E0f",
        "outputId": "f81f9687-f5d6-40a9-a569-65b3b72d41c2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.transforms.combiners._TopPerBundle'>)\n",
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n",
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Result to view Top 10 Character Names\n",
        "!echo \"Top 10 Character Names:\"\n",
        "!head -n 20 outputs_macbeth_top/part-00000-of-*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hInbxt1J_jpb",
        "outputId": "eae0e7b0-a0e6-4da0-e2c5-6946909e03c0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Character Names:\n",
            "Macbeth: 311\n",
            "Macduff: 109\n",
            "Lady: 95\n",
            "Banquo: 73\n",
            "Enter: 64\n",
            "Malcolm: 59\n",
            "What: 58\n",
            "Ross: 54\n",
            "Witch: 54\n",
            "First: 49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Results to view Word Length Distribution\n",
        "!echo \"Word Length Distribution:\"\n",
        "!cat outputs_macbeth_lengths/part-00000-of-*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PxwhTuz_lHn",
        "outputId": "84fc5d43-f4ec-4a82-908b-4e85e309f96f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Length Distribution:\n",
            "Length 7: 655\n",
            "Length 8: 165\n",
            "Length 6: 498\n",
            "Length 9: 100\n",
            "Length 4: 687\n",
            "Length 5: 515\n",
            "Length 14: 2\n",
            "Length 11: 20\n",
            "Length 10: 40\n",
            "Length 12: 1\n",
            "Length 13: 1\n"
          ]
        }
      ]
    }
  ]
}